# è‚Œç”µä¿¡å·ä¸å§¿æ€æ£€æµ‹ç»“åˆ

- è®ºæ–‡å’Œä¸“åˆ©ï¼šå’Œè€å¸ˆæƒ³çš„ä¸€æ ·ï¼Œå¤§éƒ¨åˆ†éƒ½æ˜¯ä¸¤ä¸ªæ¨¡å—çš„æ•´åˆï¼Œæ‰€ä»¥åœ¨ä»£ç æ–¹é¢æˆ‘æƒ³å…ˆæŠŠå‡ ä¸ªè¯†åˆ«æ¨¡å‹éƒ½åšä¸€ä¸‹ï¼Œè‚Œç”µä¿¡å·å’Œå§¿æ€ï¼Œæœ‰ç›‘ç£æ— ç›‘ç£ï¼Œèƒ½å¤Ÿæœ‰ä¸ªæ¯”è¾ƒï¼Œæ•´åˆæ–¹é¢ç›®å‰æ²¡æœ‰ä»€ä¹ˆå¥½æƒ³æ³•

â€‹     é“¾æ¥ï¼šhttps://pan.baidu.com/s/1_8a05BFrEKqNQ6YhiPElQg 
â€‹     æå–ç ï¼šxhhb

- ç„¶åæˆ‘ä»¬è®¡åˆ’åé¢æ¯å‘¨éƒ½å¼€ä¸ªçº¿ä¸Šä¼šåˆ†äº«ä»¥ä¸‹è¿›åº¦ï¼Œ9æœˆä¸»è¦ä»¥ä»£ç åˆ†æå’Œè®ºæ–‡ä¸ºä¸»ã€‚å› ä¸ºå¯èƒ½9ã€10æœˆä»½æœ‰ä¸€ä¸ªä¸­æœŸç­”è¾©ï¼Œå‰æœŸéœ€è¦ç´§å‡‘ä¸€ç‚¹

  æ¯å‘¨ä¸Šä¼ è¿›åº¦ï¼šhttps://trello.com/invite/b/dkYl0tQv/6671da1fc8ccac062040ca43ae807b28/semggesture-measuring

- æŒ‰ç…§å¾€å¹´ä¸­æœŸç­”è¾©ä¹‹å‰ï¼Œéœ€è¦è¿›è¡Œä¸€æ¬¡è´¹ç”¨çš„æŠ¥é”€ï¼Œæˆ‘æŸ¥äº†ä¸€ä¸‹ï¼Œå’±ä»¬åœ¨ç³»ç»Ÿé‡Œé¢æœ‰3000çš„é¢åº¦ï¼Œå¦‚æœè¦åšå§¿æ€æ£€æµ‹ï¼Œå¯ä»¥ä¹°ä¸€äº›æ‘„åƒå¤´ï¼Œè‚Œç”µæ£€æµ‹è®¾å¤‡ç­‰åˆ°æœ‰å®éªŒæ¡ä»¶ä¸‹å†è€ƒè™‘



[TOC]



## è‚Œç”µä¿¡å·

### ä»£ç 

#### æœ‰ç›‘ç£çš„å­¦ä¹ 

è‚Œç”µå‹å·æ‰‹åŠ¿è¯†åˆ« https://www.kaggle.com/code/drbilal216/emg-signal-complete-model/notebook

æ•°æ®é›† ï¼šä¿æŒåŒä¸€ä¸ªæ‰‹åŠ¿åœç•™è‹¥å¹²ç§’ï¼Œè®°å½•è‚Œç”µä¿¡å·ã€‚

##### å¯¼å…¥åŒ…ã€è¯»å–æ•°æ®

```python
import os
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

import os
for dirname, _, filenames in os.walk('/kaggle/input'):#filenames ä»£è¡¨éå†çš„, å†…å®¹æ˜¯è¯¥æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰çš„æ–‡ä»¶(ä¸åŒ…æ‹¬å­ç›®å½•)
    for filename in filenames:
         print(os.path.join(dirname, filename))
#è¯»å–æ•°æ®          
import pandas as pd
Input_path = '/kaggle/input/emg-signal-for-gesture-recognition/EMG-data.csv'
df = pd.read_csv(Input_path)#dfè¯»å–csvæ–‡ä»¶ä¸­çš„å…¨éƒ¨æ•°æ®
print(df.head())#æ‰“å°æ¯ä¸€åˆ—æ ‡é¢˜
print(df.shape)#æ‰“å°æ•°æ®çš„ï¼ˆè¡Œï¼Œåˆ—ï¼‰ 

#æ•°æ®æƒ…å†µå±•ç¤º
print("class :", df["class"].unique())#uniqueåˆ é™¤é‡å¤é¡¹ï¼Œè®©ç±»åˆ«ä»å°åˆ°å¤§æ’åˆ—
print()
print("Value Count :\n",df["class"].value_counts())#å¯¹æ¯ä¸€ä¸ªç±»åˆ«å‡ºç°æ¬¡æ•°è¿›è¡Œè®¡æ•°

#åˆ é™¤label\class\timeæ ‡ç­¾åˆ—çš„æƒ…å†µä¸‹è¾“å‡ºæ‰€æœ‰è‚Œç”µä¿¡å·æ•°å€¼
features = df.drop(columns=["label","class","time"])
display(features.head())
print(features.shape())
#å¯ä»¥é€šè¿‡.shape()ä¸å‰é¢ç¬¬ä¸€æ¬¡è¾“å‡ºè¿›è¡Œå¯¹æ¯”ï¼Œçœ‹åˆ°è¾ƒä¸ºæ˜æ˜¾çš„å·®å¼‚

#æ•°æ®ç±»å‹è¾“å‡º
print(type(Class))
print(type(features))

Class = Class.values
features = features.values

print(type(Class))
print(type(features))
```



##### æ•°æ®é¢„å¤„ç†ï¼šè®­ç»ƒé›†å’Œæµ‹è¯•æ•°æ®åˆ’åˆ†

æ­¤å¤„çš„åˆ’åˆ†ä¸ºè®­ç»ƒé›†0.7ï¼ŒéªŒè¯é›†0.1ï¼Œæµ‹è¯•é›†0.2

```python
from sklearn.model_selection import train_test_split
# 80 and 20
x_train, x_test, y_train, y_test = train_test_split(features, Class, test_size=0.2, random_state=1)

# å½’ä¸€åŒ–æ•°æ®
mean = x_train.mean(axis=0)#æ¯ä¸€åˆ—channelçš„å¹³å‡å€¼
std = x_train.std(axis=0)#æ¯ä¸€åˆ—çš„æ ‡å‡†å·®

x_train -= mean
x_train /= std

x_test -= mean
x_test /= std

# å¯¹æ ‡ç­¾yç¼–è¯‘æˆä¸€ä½çƒ­ç 
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)
```



##### å›¾åƒç»˜åˆ¶å‡½æ•°

```python
# è®­ç»ƒé›†ã€éªŒè¯é›†æŸå¤±å‡½æ•°å˜åŒ–å›¾åƒç»˜åˆ¶å‡½æ•°

def plot(loss,val_loss,acc,val_acc):
    loss = history.history['loss'] #History.history å±æ€§æ˜¯ä¸€ä¸ªè®°å½•äº†è¿ç»­è¿­ä»£çš„è®­ç»ƒ/éªŒè¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰æŸå¤±å€¼å’Œè¯„ä¼°å€¼çš„å­—å…¸
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1) #è®­ç»ƒçš„è½®æ•°

    plt.plot(epochs, loss, 'bo', label='Training loss') #trainingçš„losså€¼
    plt.plot(epochs, val_loss, 'b', label='Validation loss')#validationçš„losså€¼
    plt.title('Training and validation loss')#å›¾åƒçš„title
    plt.xlabel('Epochs')#xè½´ä¸ºè®­ç»ƒçš„è½®æ•°
    plt.ylabel('Loss')#yè½´ä¸ºè®­ç»ƒçš„loss ä¸‹é™æƒ…å†µ
    plt.legend()#å›¾æ ‡
    plt.show()#æ˜¾ç¤ºå›¾åƒ

    acc = history.history['accuracy']#å‡†ç¡®åº¦
    val_acc = history.history['val_accuracy']#åŒä¸Šä¸€ä¸ªhistory

    epochs = range(1, len(acc) + 1)

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation acc')
    plt.xlabel('Epochs')
    plt.ylabel('acc')
    plt.legend()
    plt.show()
```

![image-20220904104453947](C:\Users\86153\AppData\Roaming\Typora\typora-user-images\image-20220904104453947.png)

![image-20220904104522814](C:\Users\86153\AppData\Roaming\Typora\typora-user-images\image-20220904104522814.png)

##### æ¨¡å‹å»ºç«‹

```python
from tensorflow.keras import layers, Sequential, optimizers, Input, Model

input_tensor = Input(shape=(8,))#è¾“å…¥å½¢å¼ä¸º8åˆ—
x = layers.Dense(1024, activation='relu')(input_tensor)#è¾“å‡ºä¸º_*1024,è¾“å…¥ä¸ºinput_tensor,æ¿€æ´»å‡½æ•°ä¸ºrelu,ä»¥ä¸‹åŒç†
y = layers.Dense(512, activation='relu')(x)
z = layers.Dense(256, activation='relu')(y)
z = layers.Dense(128, activation='relu')(z)
z = layers.Dense(64, activation='relu')(z)
z = layers.Dense(32, activation='relu')(z)
z = layers.Dense(128, activation='relu')(y) # å±‚çš„éå¾ªç¯å›¾ï¼Ÿæ­¤å¤„å¯èƒ½æ˜¯å¤„äºè°ƒå‚çš„è€ƒè™‘
z = layers.Dense(64, activation='relu')(z)
z = layers.Dense(32, activation='relu')(z)
output_tensor = layers.Dense(8, activation='softmax')(z)#è¾“å‡ºå±‚

model = Model(input_tensor, output_tensor)#æ¨¡å‹å›Šæ‹¬äº†ä»inputåˆ°outputçš„æ‰€æœ‰ç½‘ç»œ

#SGD #RMSprop #Adam #Adadelta #Adagrad ##Adamax ###Nadam #Ftrl
opt = optimizers.Nadam(lr=1e-3)#ä¼˜åŒ–å™¨è®¾ç½® lrï¼šå¤§æˆ–ç­‰äº0çš„æµ®ç‚¹æ•°ï¼Œå­¦ä¹ ç‡
model.compile(optimizer = opt, 
              loss = "categorical_crossentropy",
              metrics = ["accuracy"])# ç¼–è¯‘ä¸ºå­—èŠ‚ä»£ç å¯¹è±¡ 

# ä¿å­˜æ¨¡å‹ï¼Œä¸ºtensorboaedåˆ›å»ºæ—¥å¿—ï¼Œå¹¶åº”ç”¨å°‘é‡å›è°ƒ

def callbacks(Log,Dir):
  import tensorflow as tf
  import os

  Filepath = Path
  logdir = os.path.join(Filepath, Dir)#æŠŠç›®å½•å’Œæ–‡ä»¶æ•´åˆæˆä¸ºä¸€æ¡è·¯å¾„ï¼Œæ­¤å¤„åº”è¯¥ä¸º Path/Dir
  
  callbacks_list = [tf.keras.callbacks.TensorBoard( #tensorboardç”¨æ¥æä¾›å¯è§†åŒ–å·¥å…·
                    log_dir=logdir,                 # ç”¨æ¥ä¿å­˜è¢«Tensorboaedåˆ†æçš„æ—¥å¿—æ–‡ä»¶çš„æ–‡ä»¶å     
                    histogram_freq=1,),             #å¯¹äºæ¨¡å‹ä¸­å„ä¸ªå±‚è®¡ç®—æ¿€æ´»å€¼å’Œæ¨¡å‹æƒé‡ç›´æ–¹å›¾çš„é¢‘ç‡(è®­ç»ƒè½®æ•°ä¸­)ã€‚å¦‚æœè®¾ç½®ä¸º0ï¼Œç›´æ–¹å›¾ä¸ä¼šè¢«è®¡ç®—
                    tf.keras.callbacks.EarlyStopping(   # å¦‚æœè¯¯å·®ä¸å†éšç€è®­ç»ƒå‘ç”Ÿæ˜æ˜¾æ”¹å˜ï¼Œæ­¤æ—¶ä¼šè‡ªåŠ¨ç»ˆæ­¢è®­ç»ƒ
                    monitor='val_accuracy',patience=2,),           # ç›‘æ§éªŒè¯å‡†ç¡®æ€§ monitor: è¢«ç›‘æµ‹çš„æ•°æ®'val_accuracy'æ£€éªŒé›†çš„å‡†ç¡®æ€§ï¼›patience åœ¨ç›‘æµ‹2è½®ä¸å‘ç”Ÿå˜åŒ–ï¼Œæ²¡æœ‰è¿›åº¦ååœæ­¢
                    #tf.keras.callbacks.ReduceLROnPlateau(
                    #monitor='val_loss',factor=0.1,         # lr ko .1 se multiply kerdo (kam kerdo)
                    #patience=10,),                # reduce the lrate if val loss stop improving
                    tf.keras.callbacks.ModelCheckpoint(#åœ¨æ¯ä¸ªè®­ç»ƒæœŸä¹‹åä¿å­˜æ¨¡å‹ã€‚
                    filepath= Filepath,             # ä¿å­˜è·¯å¾„
                    monitor='val_loss',             # åªä¿ç•™æœ€å¥½çš„æƒé‡å‚æ•°
                    save_best_only=True,)]
  return callbacks_list

#å¯¹â€œmodel1/my_log_dir"æ–‡ä»¶ä¸‹å­˜å‚¨çš„æ•°æ®è¿›è¡Œcallbackså‡½æ•°è®¿é—®
Path = "model1"
Dir = "my_log_dir"   
Call_B_Fun = callbacks(Path,Dir)

#æ¨¡å‹å°ºå¯¸ï¼Œå­¦ä¹ è½®æ•°
batch_size = 512            
epochs = 200                
#æ¨¡å‹æ‹Ÿåˆ
history = model.fit(x_train, y_train,
                    batch_size=batch_size, epochs = epochs,
                    validation_split = 0.2, callbacks=Call_B_Fun)
#è¯¯å·®è¿­ä»£
loss = history.history['loss']
val_loss = history.history['val_loss']
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
plot(loss,val_loss,acc,val_acc)

# ä¿å­˜æ¨¡å‹
model.save('model1/emg_1.h5')

# è½½å…¥æ•´ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸”èµ‹ç»™emg_model
from tensorflow.keras.models import load_model
emg = 'model1/emg_1.h5'
emg_model = load_model(emg)
```

##### function_3çš„æƒ…å†µ

```python
#model.layers æ˜¯åŒ…å«æ¨¡å‹ç½‘ç»œå±‚çš„å±•å¹³åˆ—è¡¨ã€‚
#model.inputs æ˜¯æ¨¡å‹è¾“å…¥å¼ é‡çš„åˆ—è¡¨ã€‚
#model.outputs æ˜¯æ¨¡å‹è¾“å‡ºå¼ é‡çš„åˆ—è¡¨ã€‚
#model.summary() æ‰“å°å‡ºæ¨¡å‹æ¦‚è¿°ä¿¡æ¯ã€‚ å®ƒæ˜¯ utils.print_summary çš„ç®€æ·è°ƒç”¨ã€‚

new_model = Model(emg_model.inputs, emg_model.layers[-2].output) # removing layers
new_model.summary()
# removed all layers except conv
```

åœ¨fun3çš„åŸºç¡€ä¸ŠåŠ ä¸€å±‚ç”Ÿæˆfun5

```python
from tensorflow.keras import layers, optimizers, Input, Model

input_tensor = Input(shape=(8,))#è¾“å…¥å¡‘æ€§
x = new_model(input_tensor)       # fun3
#z = layers.Dense(256, activation='relu')(x)
output_tensor = layers.Dense(8, activation='softmax')(x)#æ–°çš„è¾“å‡ºå±‚ï¼Œsoftmaxä¸ºæ¿€æ´»å‡½æ•°

model = Model(input_tensor, output_tensor)#æ ¹æ®å·²çŸ¥è¾“å…¥è¾“å‡ºå»ºç«‹æ¨¡å‹

# ä¼ å…¥ä¼˜åŒ–å™¨åç§°: é»˜è®¤å‚æ•°å°†è¢«é‡‡ç”¨
opt = optimizers.Nadam(lr=1e-3)
model.compile(optimizer = opt, 
              loss = "categorical_crossentropy",
              metrics = ["accuracy"])

model.summary()
```



åæ–‡é‡å¤å‰è¿°å‡½æ•°ï¼Œä¸å†èµ˜è¿°

## å§¿æ€æ£€æµ‹ï¼š

### æ–‡çŒ®

#### Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields

doi:https://ieeexplore.ieee.org/document/8099626

##### Abstract

æå‡ºäº†ä¸€ç§æœ‰æ•ˆæ£€æµ‹å›¾åƒä¸­å¤šäººçš„2Då§¿åŠ¿çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨éå‚æ•°è¡¨ç¤ºï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºPAFSï¼Œä»¥å­¦ä¹ å°†**èº«ä½“éƒ¨ä½ä¸å›¾åƒä¸­çš„ä¸ªä½“**ç›¸å…³è”ã€‚è¯¥ä½“ç³»ç»“æ„æ˜¯ä¸€ä¸ªè´ªå¿ƒçš„**è‡ªä¸‹è€Œä¸Šè§£ææ­¥éª¤**ï¼Œè¯¥æ­¥éª¤åœ¨å®ç°å®æ—¶æ€§èƒ½çš„åŒæ—¶ä¿æŒè¾ƒé«˜çš„ç²¾åº¦ï¼Œ**è€Œä¸å›¾åƒä¸­çš„äººæ•°æ— å…³**ã€‚å…ˆé”å®šè‚¢ä½“ï¼Œè‡ªä¸‹è€Œä¸ŠåŒºåˆ†å¤šä¸ªäººä½“ï¼Œä¸è§†é‡èŒƒå›´å†…äººæ•°æ— å…³ï¼Œèƒ½å¤Ÿä¿éšœå®æ—¶è®¡ç®—ä¸­çš„æ•ˆç‡ã€‚

##### Introduction

###### **è‡ªä¸Šè€Œä¸‹çš„ç»“æ„ï¼šå…ˆç¡®å®šå•ä¸ªäººä½“å†ç¡®å®šèº¯å¹²**

<u>å­˜åœ¨é—®é¢˜ï¼š</u>

1. é¦–å…ˆï¼Œæ¯ä¸ªå›¾åƒå¯èƒ½åŒ…å«å¯èƒ½åœ¨ä»»ä½•ä½ç½®æˆ–åˆ»åº¦ä¸Šå‘ç”Ÿçš„äººæ•°æœªçŸ¥æ•°ã€‚
2. å…¶æ¬¡ï¼Œç”±äºæ¥è§¦ï¼Œé®æŒ¡å’Œè‚¢ä½“å…³èŠ‚ï¼Œäººä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨å¼•èµ·å¤æ‚çš„ç©ºé—´å¹²æ‰°ï¼Œä»è€Œä½¿é›¶ä»¶çš„å…³è”å˜å¾—å›°éš¾ã€‚
3. ç¬¬ä¸‰ï¼Œè¿è¡Œæ—¶çš„å¤æ‚æ€§å¾€å¾€ä¼šéšå›¾åƒä¸­çš„äººæ•°è€Œå¢é•¿ï¼Œä»è€Œä½¿å®æ—¶æ€§èƒ½æˆä¸ºæŒ‘æˆ˜ã€‚
4. å½“äººåƒè·ç¦»æ‘„åƒå¤´è¿‡è¿‘æ—¶å­˜åœ¨è¯†åˆ«å¤±æ•ˆé—®é¢˜ï¼Œè¿™ç§é—®é¢˜æ— æ³•é€šè¿‡ç®—æ³•ä¿®å¤

###### è‡ªä¸‹è€Œä¸Šç»“æ„ï¼šå…ˆå¯¹èº¯å¹²è¿›è¡Œå‘é‡åŒ–ï¼Œè¿›è€Œæ¨å¯¼æ•´ä¸ªäººä½“

![image-20221010205010253](C:\Users\86153\AppData\Roaming\Typora\typora-user-images\image-20221010205010253.png)

<u>è§£å†³é—®é¢˜ï¼š</u>

1. æ—©æœŸé²æ£’æ€§ï¼Œç®—æ³•å¤æ‚åº¦ä¸è§†é‡ä¸­äººæ•°å…³ç³»å¼±

2. è¯¥ä½“ç³»ç»“æ„é€šè¿‡ç›¸åŒé¡ºåºé¢„æµ‹è¿‡ç¨‹çš„ä¸¤ä¸ªåˆ†æ”¯å…±åŒå­¦ä¹ é›¶ä»¶ä½ç½®åŠå…¶å…³è”

   ä½†ä¹Ÿå­˜åœ¨ä»partæ¨å¯¼å…¨å±€çš„ç®—æ³•å¤æ‚æ€§ï¼ŒNPç±»é—®é¢˜ï¼Œé€šè¿‡åŸºäºé›¶ä»¶çš„ResNetï¼ˆæ®‹å·®ç¥ç»ç½‘ç»œï¼‰

æ–‡ç« å°†ä»‹ç»ä¸€ç§åŸºäºè´ªå¿ƒè‡ªä¸‹è€Œä¸Šçš„ç®—æ³•ï¼Œé¦–å…ˆæˆ‘ä»¬é€šè¿‡PAFSå¾—åˆ°ä¸€ç»„2DçŸ¢é‡å­—æ®µï¼Œè¯¥åœºåœ¨å›¾åƒåŸŸä¸Šç¼–ç äº†å››è‚¢çš„ä½ç½®å’Œæ–¹å‘ã€‚åŒæ—¶é€šè¿‡è‡ªä¸‹è€Œä¸Šçš„ç¼–ç å¯¹å…¨å±€è¿›è¡Œé¢„æµ‹ï¼Œä»¥å…è®¸è´ªå¿ƒçš„è§£æä»¥è®¡ç®—æˆæœ¬çš„ä¸€å°éƒ¨åˆ†è·å¾—é«˜è´¨é‡çš„ç»“æœã€‚

å¼€æºä»£ç ï¼šhttps://github.com/CMU-Perceptual-Computing-Lab/openpose



##### Method

![image-20221011230053860](C:\Users\86153\AppData\Roaming\Typora\typora-user-images\image-20221011230053860.png)

è¾“å…¥ï¼šw*hçš„å›¾åƒ

è¾“å‡ºï¼š2Dçš„å…³èŠ‚ä½ç½®

è¿‡ç¨‹ï¼š

### ä»£ç 

#### å§¿æ€è¯†åˆ«åŒ…åº”ç”¨â€”è§†é¢‘è½¬åŒ–ä¸ºæ¡†æ¶

GitHubåŒ…ï¼šhttps://github.com/tryagainconcepts/tf-pose-estimation/blob/master/README.md

æºä»£ç é“¾æ¥ï¼šhttps://www.kaggle.com/code/kmader/running-pose-estimate

##### å¯¼å…¥æ•°æ®ã€åŒ…ã€æ•°æ®è¯»å–å‡½æ•°

```python
%load_ext autoreload
%autoreload 2
import seaborn as sns# å¯è§†åŒ–å·¥å…·
import matplotlib.pyplot as plt#ç»˜å›¾å·¥å…·
#å…³äºä¸€äº›ç•Œé¢çš„æ˜¾ç¤ºé£æ ¼è®¾ç½®
plt.rcParams["figure.figsize"] = (8, 8)#ä¸æ”¹å˜åˆ†è¾¨ç‡çš„æƒ…å†µä¸‹ï¼Œæ”¹å˜å›¾ç‰‡å°ºå¯¸åˆ°8*8
plt.rcParams["figure.dpi"] = 125#åˆ†è¾¨ç‡
plt.rcParams["font.size"] = 14#æ–‡å­—å°ºå¯¸
plt.rcParams['font.family'] = ['sans-serif']#å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.style.use('ggplot')
sns.set_style("whitegrid", {'axes.grid': False})

%matplotlib inline
#ä¸€äº›å…³äºå›¾åƒç›‘æµ‹çš„åº“
import tf_pose
import cv2
from glob import glob
from tqdm import tqdm_notebook
from PIL import Image
import numpy as np
import os
#è¯»å–è§†é¢‘çš„å‡½æ•°
def video_gen(in_path):
    c_cap = cv2.VideoCapture(in_path)#æ‰“å¼€æ‘„åƒå¤´è·å–è§†é¢‘æ•°æ®
    while c_cap.isOpened():#ç›‘æµ‹ä»¥å±äºæ‰“å¼€çŠ¶æ€ï¼Œè¯»å–è§†é¢‘ä¿¡æ¯
        ret, frame = c_cap.read()
        #è¯»å–ä¸¤ä¸ªè¿”å›å€¼
        #retæ˜¯å¸ƒå°”å€¼ï¼Œè¯»å–å¸§æ•°ä¸ºæ­£è¿”å›Trueï¼Œè¯»å–åˆ°è§†é¢‘æœ«å°¾è¿”å›å€¼ä¸ºFalse
        #frameå°±æ˜¯æ¯ä¸€å¸§çš„ç”»é¢
        if not ret:
            break
        yield c_cap.get(cv2.CAP_PROP_POS_MSEC), frame[:, :, ::-1]
        #cv2.CAP_PROP_POS_MSECè§†é¢‘æ–‡ä»¶çš„å½“å‰ä½ç½®ï¼ˆæ’­æ”¾ï¼‰ä»¥æ¯«ç§’ä¸ºå•ä½
    c_cap.release()#åŠæ—¶é‡Šæ”¾èµ„æº
    
video_paths = glob('../input/*.mp4')#è§†é¢‘è·¯å¾„
c_video = video_gen(video_paths[0])#ä¾æ¬¡è¯»å–è§†é¢‘æ–‡ä»¶
for _ in range(300):
    c_ts, c_frame = next(c_video)
plt.imshow(c_frame)


```



##### å§¿æ€ä¼°è®¡åŒ…çš„æ•ˆæœ

```python
#å¯¼å…¥å§¿æ€ç›‘æµ‹åŒ…ï¼Œå¯ä»¥ç›´æ¥å¯¹å›¾åƒå’Œè§†é¢‘è¿›è¡Œå¤„ç†
from tf_pose.estimator import TfPoseEstimator
from tf_pose.networks import get_graph_path, model_wh
tfpe = tf_pose.get_estimator()

humans = tfpe.inference(npimg=c_frame, upsample_size=4.0)#å¯¹hunmanå˜é‡èµ‹äºˆè¯»å–åˆ°çš„c_frameå€¼
print(humans)

#å¯¹å›¾åƒä¸Šçš„äººå§¿æ€è¿›è¡Œä¼°è®¡
new_image = TfPoseEstimator.draw_humans(c_frame[:, :, ::-1], humans, imgcopy=False)
fig, ax1 = plt.subplots(1, 1, figsize=(10, 10))
ax1.imshow(new_image[:, :, ::-1])#æ˜¾ç¤ºè§†é¢‘çš„æœ€åä¸€å¸§

#lambda å‚æ•°åˆ—è¡¨:è¿”å›å€¼ æ­¤å¤„ä»£è¡¨c_figå¯¹åº”çš„å‡½æ•°çš„è¿”å›å€¼
#format()åŠŸèƒ½å¾ˆå¼ºå¤§ï¼Œå®ƒæŠŠå­—ç¬¦ä¸²å½“æˆä¸€ä¸ªæ¨¡æ¿ï¼Œé€šè¿‡ä¼ å…¥çš„å‚æ•°è¿›è¡Œæ ¼å¼åŒ–ï¼Œå¹¶ä¸”ä½¿ç”¨å¤§æ‹¬å·â€˜{}â€™ä½œä¸ºç‰¹æ®Šå­—ç¬¦ä»£æ›¿â€˜%â€™ã€‚
#æ­¤å¤„å¯ä»¥ç†è§£ä¸ºbp_{k}_{vec_name}
#zip() å‡½æ•°ç”¨äºå°†å¯è¿­ä»£çš„å¯¹è±¡ä½œä¸ºå‚æ•°ï¼Œå°†å¯¹è±¡ä¸­å¯¹åº”çš„å…ƒç´ æ‰“åŒ…æˆä¸€ä¸ªä¸ªå…ƒç»„ï¼Œç„¶åè¿”å›ç”±è¿™äº›å…ƒç»„ç»„æˆçš„åˆ—è¡¨ã€‚
#æ­¤å¤„è¾“å‡ºæ¯ä¸€ä¸ªéƒ¨ä½çš„åæ ‡ä¸å¾—åˆ†
body_to_dict = lambda c_fig: {'bp_{}_{}'.format(k, vec_name): vec_val 
                              for k, part_vec in c_fig.body_parts.items() 
                              for vec_name, vec_val in zip(['x', 'y', 'score'],
                                                           (part_vec.x, 1-part_vec.y, part_vec.score))}
c_fig = humans[0]
body_to_dict(c_fig)

MAX_FRAMES = 200
#åˆ›å»ºä¸€ä¸ªèº«ä½“å§¿æ€å­—å…¸
body_pose_list = []
for vid_path in tqdm_notebook(video_paths, desc='Files'):
    c_video = video_gen(vid_path)#è¯»å–è§†é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨ä¹‹å‰å®šä¹‰çš„å‡½æ•°video_gen
    c_ts, c_frame = next(c_video)#ä¿¡æ¯æŒ‡é’ˆè½¬å‘ä¸‹ä¸€ä¸ªè§†é¢‘æ–‡ä»¶
    out_path = '{}_out.avi'.format(os.path.split(vid_path)[1])#æ ‡è®°è§†é¢‘æ–‡ä»¶çš„å­˜å‚¨è·¯å¾„,"os.path.split(vid_path)[1]_out.avi"
    
    out = cv2.VideoWriter(out_path,
                          cv2.VideoWriter_fourcc('M','J','P','G'),#æ–‡ä»¶å­˜å‚¨æ ¼å¼MJPG,æŒ‡å®šè§†é¢‘å¤§å°ä¸º10
                          10, 
                          (c_frame.shape[1], c_frame.shape[0]))
    for (c_ts, c_frame), _ in zip(c_video, 
                                  tqdm_notebook(range(MAX_FRAMES), desc='Frames')):
        bgr_frame = c_frame[:,:,::-1]
        humans = tfpe.inference(npimg=bgr_frame, upsample_size=4.0)#å‹ç¼©å›¾ç‰‡
        for c_body in humans:
            body_pose_list += [dict(video=out_path, time=c_ts, **body_to_dict(c_body))]#å­˜å‚¨ä¿¡æ¯
        new_image = TfPoseEstimator.draw_humans(bgr_frame, humans, imgcopy=False)
        out.write(new_image)
    out.release()
#zip() å‡½æ•°ç”¨äºå°†å¯è¿­ä»£çš„å¯¹è±¡ä½œä¸ºå‚æ•°ï¼Œå°†å¯¹è±¡ä¸­å¯¹åº”çš„å…ƒç´ æ‰“åŒ…æˆä¸€ä¸ªä¸ªå…ƒç»„ï¼Œç„¶åè¿”å›ç”±è¿™äº›å…ƒç»„ç»„æˆçš„åˆ—è¡¨ã€‚
```



##### å¯è§†åŒ–å¯¹æ¯”ä¸¤æ®µè§†é¢‘çš„å·®å¼‚æ€§

```python
import pandas as pd
body_pose_df = pd.DataFrame(body_pose_list)
body_pose_df.describe()

fig, m_axs = plt.subplots(1, 2, figsize=(15, 5))
for c_ax, (c_name, c_rows) in zip(m_axs, body_pose_df.groupby('video')):
    #éšç€æ—¶é—´å˜åŒ–ä¸åŒèº¯å¹²çš„Yå˜åŒ–
    for i in range(17):
        c_ax.plot(c_rows['time'], c_rows['bp_{}_y'.format(i)], label='x {}'.format(i))
    c_ax.legend()
    c_ax.set_title(c_name)
    
fig, m_axs = plt.subplots(1, 2, figsize=(15, 5))
for c_ax, (c_name, n_rows) in zip(m_axs, body_pose_df.groupby('video')):
    for i in range(17):
        #éšç€æ—¶é—´å˜åŒ–ä¸åŒèº¯å¹²çš„bp_å¾—åˆ†å˜åŒ–
        c_rows = n_rows.query('bp_{}_score>0.6'.format(i)) # only keep confident results
        c_ax.plot(c_rows['bp_{}_x'.format(i)], c_rows['bp_{}_y'.format(i)], label='BP {}'.format(i))
    c_ax.legend()
    c_ax.set_title(c_name)
    
body_pose_df.to_csv('body_pose.csv', index=False)
```







#### æœ‰ç›‘ç£çš„å­¦ä¹ -CNN

ä»¥æ‰‹åŠ¿çº¢å¤–å›¾ç‰‡ä¸ºä¾‹https://www.kaggle.com/code/benenharrington/hand-gesture-recognition-database-with-cnn/notebook?q=gesture+recognition

æ•°æ®é›†ï¼šæ‰‹åŠ¿è¯†åˆ«æ•°æ®åº“æ˜¯åç§ä¸åŒæ‰‹åŠ¿çš„è¿‘çº¢å¤–å›¾åƒçš„é›†åˆã€‚åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ¥ä¸ºè¿™äº›å›¾åƒæ„å»ºåˆ†ç±»å™¨ã€‚ æˆ‘ä»¬å°†é¦–å…ˆåŠ è½½ä¸€äº›è¯»å–å’Œç»˜åˆ¶å›¾åƒæ‰€éœ€çš„åŒ…ã€‚

é“¾æ¥ğŸ”—https://www.kaggle.com/datasets/gti-upm/leapgestrecog

##### å¯¼å…¥åŒ…

```python
import numpy as np
import os #è®¿é—®æ–‡ä»¶
from PIL import Image # å¤„ç†å›¾ç‰‡
import matplotlib.pyplot as plt
import matplotlib.image as mpimg # æ˜¾ç¤ºå›¾ç‰‡

```

##### å»ºç«‹å­—å…¸

å¦‚æ•°æ®æ¦‚è¿°ä¸­æ‰€è¿°ï¼Œæœ‰ 10 ä¸ªæ–‡ä»¶å¤¹æ ‡è®°ä¸º 00 åˆ° 09ï¼Œæ¯ä¸ªæ–‡ä»¶å¤¹åŒ…å«æ¥è‡ªç»™å®šä¸»é¢˜çš„å›¾åƒã€‚åœ¨æ¯ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œæ¯ä¸ªæ‰‹åŠ¿éƒ½æœ‰å­æ–‡ä»¶å¤¹ã€‚æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªå­—å…¸æŸ¥æ‰¾æ¥å­˜å‚¨æˆ‘ä»¬éœ€è¦è¯†åˆ«çš„æ‰‹åŠ¿åç§°ï¼Œå¹¶ä¸ºæ¯ä¸ªæ‰‹åŠ¿æä¾›ä¸€ä¸ªæ•°å­—æ ‡è¯†ç¬¦ã€‚æˆ‘ä»¬è¿˜å°†æ„å»ºä¸€ä¸ªå­—å…¸åå‘æŸ¥æ‰¾ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆæ‰‹åŠ¿ä¸ç»™å®šçš„æ ‡è¯†ç¬¦ç›¸å…³è”ã€‚

```python
lookup = dict()#å»ºç«‹å­—å…¸
reverselookup = dict()#å»ºç«‹é€†å­—å…¸
count = 0
for j in os.listdir('../input/leapgestrecog/leapGestRecog/00/'):
    if not j.startswith('.'): #*é˜²æ­¢è¢«éšè—çš„æ–‡ä»¶è¢«è®¿é—®
        lookup[j] = count #å­—å…¸å½¢å¼ æ–‡ä»¶åï¼šåºå·
        reverselookup[count] = j#é€†å­—å…¸å½¢å¼ åºå·ï¼šæ–‡ä»¶
        count = count + 1
```

##### å›¾åƒé¢„å¤„ç†

çº¢å¤–ä¼ æ„Ÿå™¨é‡‡é›†å›¾åƒè¿‡å¤§ï¼Œè½¬æ¢ä¸ºç°åº¦å¹¶è°ƒæ•´å¤§å°ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªéå†æ–‡ä»¶çš„è¿‡ç¨‹

```python
x_data = [] #å›¾åƒé›†åˆ
y_data = [] #åˆ†ç±»é›†åˆ
datacount = 0 # å›¾ç‰‡è®¡æ•°å™¨
for i in range(0, 10): # è®¿é—®åä¸ªæ–‡ä»¶
    for j in os.listdir('../input/leapgestrecog/leapGestRecog/0' + str(i) + '/'):#å°†iå­—ç¬¦ä¸²ï¼Œè¿™æ ·èƒ½å¤Ÿå¾ªç¯è¡¨è¾¾æ–‡ä»¶åç§°ï¼Œå¾ªç¯è®¿é—®
        if not j.startswith('.'): # åŒæ ·ä¸ºäº†é¿å…è®¿é—®åˆ°éšè—æ–‡ä»¶
            count = 0  
            for k in os.listdir('../input/leapgestrecog/leapGestRecog/0' + 
                                str(i) + '/' + j + '/'):
                                
                img = Image.open('../input/leapgestrecog/leapGestRecog/0' + 
                                 str(i) + '/' + j + '/' + k).convert('L')
                                # è®¿é—®'0i'æ–‡ä»¶ä¸­çš„æ¯ä¸€é¡¹ï¼Œå¹¶å°†å…¶ç°åº¦å›¾åƒèµ‹äºˆimg
                img = img.resize((320, 120))#è°ƒæ•´å›¾åƒå°ºå¯¸
                arr = np.array(img)#çŸ©é˜µèµ‹å€¼
                x_data.append(arr) #åœ¨xä¸­å­˜å‚¨ç°åº¦å€¼
                count = count + 1
            y_values = np.full((count, 1), lookup[j]) #å¯¹äºcountå³oiæ–‡ä»¶ä¸­æ‰€æœ‰çš„çš„å›¾åƒçš„ç±»åˆ«æ ‡è®°ä¸ºå­—å…¸lookupä¸­çš„ç±»åˆ«
            y_data.append(y_values)
            datacount = datacount + count#æ€»æ•°=æ€»æ•°+å½“å‰æ–‡ä»¶å›¾ç‰‡æ•°
x_data = np.array(x_data, dtype = 'float32')
y_data = np.array(y_data)
y_data = y_data.reshape(datacount, 1) # æ”¹å˜ä¸€ä¸‹yçš„å½¢çŠ¶
```

##### æ•°æ®é¢„å¤„ç†

```python
import keras
from keras.utils import to_categorical
y_data = to_categorical(y_data)#æŠŠæ•°æ®å˜æˆäºŒè¿›åˆ¶

x_data = x_data.reshape((datacount, 120, 320, 1))#é‡å¡‘x_dataçš„å½¢çŠ¶ï¼Œä½¿å®ƒçš„å€¼ä½äº0å’Œ1ä¸­é—´
x_data /= 255

#è®­ç»ƒé›†ä¸æµ‹è¯•é›†åˆ’åˆ†
from sklearn.model_selection import train_test_split
x_train,x_further,y_train,y_further = train_test_split(x_data,y_data,test_size = 0.2)
x_validate,x_test,y_validate,y_test = train_test_split(x_further,y_further,test_size = 0.5)

```

##### æ¨¡å‹è®­ç»ƒ

kearsæœ‰ä¸¤ç§ä¸åŒçš„å»ºæ¨¡æ–¹æ³•

- Sequential models

  Sequentialæ¨¡å‹å­—é¢ä¸Šçš„ç¿»è¯‘æ˜¯é¡ºåºæ¨¡å‹ï¼Œç»™äººçš„ç¬¬ä¸€æ„Ÿè§‰æ˜¯é‚£ç§ç®€å•çš„çº¿æ€§æ¨¡å‹ï¼Œä½†å®é™…ä¸ŠSequentialæ¨¡å‹å¯ä»¥æ„å»ºéå¸¸å¤æ‚çš„ç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬å…¨è¿æ¥ç¥ç»ç½‘ç»œã€å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ã€å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)ã€ç­‰ç­‰ã€‚è¿™é‡Œçš„Sequentialæ›´å‡†ç¡®çš„åº”è¯¥ç†è§£ä¸ºå †å ï¼Œé€šè¿‡å †å è®¸å¤šå±‚ï¼Œæ„å»ºå‡ºæ·±åº¦ç¥ç»ç½‘ç»œã€‚

  Sequentialæ¨¡å‹çš„æ ¸å¿ƒæ“ä½œæ˜¯æ·»åŠ layersï¼ˆå›¾å±‚ï¼‰

- Functional API

```python
from keras import layers
from keras import models

model=models.Sequential()
#è¾“å…¥å±‚
model.add(layers.Conv2D(32, (5, 5), strides=(2, 2),activation='relu', input_shape=(120, 320,1))) #æ·»åŠ ä¸€ä¸ªå¸¦æœ‰32ä¸ª5*5çš„è¿‡æ»¤å™¨çš„å·ç§¯å±‚ï¼Œæ°´å¹³æ»‘åŠ¨æ­¥é•¿å’Œå‚ç›´æ»‘åŠ¨æ­¥é•¿éƒ½ä¸º2ï¼Œå…¶è·å–120* 320 * 1çš„è¾“å…¥å›¾åƒ

#æœ€å¤§æ± åŒ–å±‚ æŠŠå›¾åƒå‹ç¼©ä»5*5åˆ°2*2 é€‰æ‹©æœ€å¤§å€¼
model.add(layers.MaxPooling2D((2, 2)))
#é‡å¤å‹ç¼©æ“ä½œ
model.add(layers.Conv2D(64, (3, 3), activation='relu')) 
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
#å±•å¹³å±‚ï¼Œæ±‚å’Œå¹³å‡
model.add(layers.Flatten())
#å…¨è¿æ¥å±‚ï¼šå’Œä¸Šä¸€å±‚æ•°æ®é‡ä¸€æ ·
model.add(layers.Dense(128, activation='relu'))#(ç¥ç»å…ƒèŠ‚ç‚¹æ•°ï¼Œæ¿€æ´»å‡½æ•°)
model.add(layers.Dense(10, activation='softmax'))
```

##### å»ºæ¨¡æˆåŠŸ

optimizer ä¼˜åŒ–å™¨
åŸºäºè®­ç»ƒæ•°æ®å’ŒæŸå¤±å‡½æ•°æ¥æ›´æ–°ç½‘ç»œçš„æœºåˆ¶ï¼Œå¸¸ç”¨çš„æœ‰Adamï¼Œ RMSpropã€SGDç­‰

loss æŸå¤±å‡½æ•°
ç½‘ç»œè¡¡é‡åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„æ€§èƒ½ï¼Œå³ç½‘ç»œå¦‚ä½•æœç€æ­£ç¡®çš„æ–¹å‘å‰è¿›ã€‚
BinaryCrossentropy, CategoricalCrossentropy,KLDivergenceç­‰

metrics ç›‘æ§æŒ‡æ ‡
è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­éœ€è¦ç›‘æ§çš„æŒ‡æ ‡ã€‚å¸¸ç”¨çš„æœ‰AUCã€Accuracyã€BinaryAccuracyã€BinaryCrossentropy, CategoricalCrossentropy, KLDivergenceã€Precisionç­‰ç­‰

losså’Œmetricså…³ç³»
losså’Œmetricséƒ½æ˜¯ç”¨æ¥è¯„ä»·è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ï¼›
optimizeræ˜¯æ ¹æ®losså€¼è¿›è¡Œåå‘è¯¯å·®æ–¹å‘ä¼ æ’­ï¼Œè®¡ç®—æ›´æ–°ç½‘ç»œæƒå€¼ï¼›
metricsä¸å‚ä¸ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ï¼Œåªä½œä¸ºä¸€ä¸ªç›‘æ§æŒ‡æ ‡ï¼Œæ–¹ä¾¿ç›´è§‚æ˜¾ç¤ºæ¨¡å‹çš„é¢„æµ‹ï¼Œé€‰æ‹©èŒƒå›´ç›¸æ¯”lossæ›´å¤šï¼›
æ¯”å¦‚åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œäº¤å‰ç†µæ˜¯æ¨¡å‹è®­ç»ƒçš„lossï¼Œä½†æ˜¯æˆ‘ä»¬éš¾ä»¥ç›´è§‚é€šè¿‡äº¤å‰ç†µçš„æ•°å€¼è¿›è¡Œåˆ¤æ–­ï¼Œéœ€è¦ä¸€ä¸ªæ›´ä¸ºç›´è§‚çš„è´¨ä¿ï¼Œå› æ­¤é€‰æ‹©ç²¾åº¦ä½œä¸ºmetrics

```python
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1, validation_data=(x_validate, y_validate))
#(è¾“å…¥æ•°æ®ï¼Œæ ‡ç­¾ï¼Œè®­ç»ƒæ€»è½®æ•°=10ï¼Œæ¢¯åº¦ä¸‹é™æ¯ä¸ªbatchæ ·æœ¬æ•°=64ï¼Œè¾“å‡ºè¿›åº¦æ¡è®°å½•ï¼ŒéªŒè¯é›†)

#è¯¯å·®è®¡ç®—
[loss, acc] = model.evaluate(x_test,y_test,verbose=1)
print("Accuracy:" + str(acc))
```

